{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt một số hàm tiền xử lý văn bản cần thiết\n",
    "import regex as re\n",
    "from underthesea import word_tokenize\n",
    " \n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    " \n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "# Hàm chuyển Unicode dựng sẵn về Unicde tổ hợp (phổ biến hơn)\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def remove_html(txt):\n",
    "    return re.sub(r'<[^>]*>', '', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(document):\n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
    "    # tách từ\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# mo file json\n",
    "with open('train.json', 'r',encoding='utf8') as openfile:\n",
    "    json_object = json.load(openfile) #  ham .load() tra ve mot list\n",
    "\n",
    "\n",
    "lst = []\n",
    "for i in range(len(json_object)):\n",
    "    tmp = str(json_object[i]['label']) + \" \" +json_object[i]['question'] +\" \"+ json_object[i]['title'] +\" \"+ json_object[i]['text']\n",
    "    lst.append(tmp)\n",
    "with open('train.txt', 'w',encoding='utf8') as f:\n",
    "    for item in lst:\n",
    "        label = item.split()[0]\n",
    "        text = \" \".join(item.split()[1:])\n",
    "        text = text_preprocess(text)\n",
    "        item = label + \" \"+ text\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "source": [
    "## Thống kê số lượng câu môi nhãn "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True 5738\nFalse 12370\n"
     ]
    }
   ],
   "source": [
    "filepath = 'train.txt'\n",
    "count = {}\n",
    "with open(filepath,encoding=\"utf-8\") as fp:\n",
    "    for line in fp.readlines():\n",
    "        key = line.split()[0]\n",
    "        count[key] = count.get(key, 0) + 1\n",
    "\n",
    "for key in count:\n",
    "    print(key, count[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "là 11449\ncủa 7675\nvà 6926\nnăm 5378\nnam 4839\nđược 4701\ncó 4146\nviệt 3926\ntrong 3789\nmột 3473\ncác 3101\nở 3039\nngười 2765\nnào 2553\nvới 2535\nvào 2131\ntừ 2122\nnhất 2075\ntháng 1990\nngày 1929\nnước 1882\ntên 1807\nđã 1773\ntrên 1739\ntỉnh 1697\ncho 1642\ntại 1617\nđến 1480\nông 1402\nnhững 1375\nthành_phố 1342\nthế_giới 1338\nthứ 1266\nnày 1235\nđông 1225\nkhi 1212\nquốc_gia 1207\nvề 1203\ntheo 1178\nra 1158\nphía 1157\nnhà 1106\nhồ 1064\nlớn 1060\ncòn 1057\nđầu_tiên 1007\nsau 999\nthuộc 978\nsông 960\nđó 957\ngì 953\nnằm 946\ntiếng 943\nhai 942\nai 931\ngọi 916\nbắc 912\nnhiều 902\n1 867\nkhông 828\nvua 823\ncao 803\nhoa 801\n2 799\ncũng 786\nđể 785\nnhư 758\ntây 742\nhay 723\nquốc 707\nkhoảng 704\nđây 698\nnúi 688\ndo 680\nanh 675\nlàm 661\nbao_nhiêu 654\nkhác 653\nbiển 632\nbộ 613\nlịch_sử 600\nsự 596\nvùng 595\nthành 594\nsinh 592\nchính_thức 590\n3 583\nlại 580\ndân_số 567\nhuyện 567\nba 561\n5 560\nkỳ 559\ndiện_tích 546\nđảo 545\ntrung_quốc 538\ntổ_chức 537\nlần 528\n4 523\ntrước 515\n"
     ]
    }
   ],
   "source": [
    "# Thống kê các word xuất hiện ở tất cả các nhãn\n",
    "total_label = 2\n",
    "vocab = {}\n",
    "label_vocab = {}\n",
    "with open('train.txt',encoding=\"utf-8\") as fp:\n",
    "    for line in fp.readlines():\n",
    "        words = line.split()\n",
    "        # lưu ý từ đầu tiên là nhãn\n",
    "        label = words[0]\n",
    "        if label not in label_vocab:\n",
    "            label_vocab[label] = {}\n",
    "        for word in words[1:]:\n",
    "            label_vocab[label][word] = label_vocab[label].get(word, 0) + 1\n",
    "            if word not in vocab:\n",
    "                vocab[word] = set()\n",
    "            vocab[word].add(label)\n",
    "\n",
    "count = {}\n",
    "for word in vocab:\n",
    "    if len(vocab[word]) == total_label:\n",
    "        count[word] = min([label_vocab[x][word] for x in label_vocab])\n",
    "        \n",
    "sorted_count = sorted(count, key=count.get, reverse=True)\n",
    "for word in sorted_count[:100]:\n",
    "    print(word, count[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loại stopword khỏi dữ liệu\n",
    "# lưu file dùng về sau\n",
    "stopword = set()\n",
    "with open('stopwords.txt','w',encoding=\"utf-8\") as fp:\n",
    "    for word in sorted_count[:100]:\n",
    "        stopword.add(word)\n",
    "        fp.write(word + '\\n')\n",
    "    \n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)\n",
    "    \n",
    "    \n",
    "with open('trainRemovedStopwords.prep', 'w',encoding=\"utf-8\") as fp:\n",
    "    for line in open('train.txt',encoding=\"utf-8\"):\n",
    "        line = remove_stopwords(line)\n",
    "        fp.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['False', 'True'] \n\nnhập asean hiệp_hội á quan_hệ mậu_dịch giữa asean tính chung 1990 tới nay tốc_độ tăng trung_bình 26 8 hiện_nay buôn_bán asean chiếm 23 tổng buôn_bán quốc_tế ta mặt_hàng xuất_khẩu chính ta sang asean gạo indonesia thị_trường gạo hiệp_hội tiếp philippines malaysia hàng_hóa nhập_khẩu asean chủ_yếu nguyên_liệu xuất_khẩu xăng dầu phân_bón thuốc_trừ_sâu hàng điện_tử 0 \n\nlê lợi lê_lai quan_hệ lê thái_tổ lê thái_tổ ngôi thì qua_đời 22 8 âm_lịch 7 9 dương_lịch quý sửu 1433 hưởng dương 49 tuổi vì nhớ công lê_lai chết thay mình chí_linh trước_kia dặn đời phải giỗ lê_lai giỗ bởi_thế đời truyền câu hăm mốt lê_lai hăm lê_lợi 0\n"
     ]
    }
   ],
   "source": [
    "# Chia tập train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "test_percent = 0.2\n",
    "\n",
    "text = []\n",
    "label = []\n",
    "\n",
    "for line in open('train.prep',encoding=\"utf-8\"):\n",
    "    words = line.strip().split()\n",
    "    label.append(words[0])\n",
    "    text.append(' '.join(words[1:]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=test_percent, random_state=42)\n",
    "\n",
    "# Lưu train/test data\n",
    "# Giữ nguyên train/test để về sau so sánh các mô hình cho công bằng\n",
    "with open('trainSauKhiDaChia.txt', 'w',encoding=\"utf-8\") as fp:\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        fp.write('{} {}\\n'.format(y, x))\n",
    "\n",
    "with open('testSauKhiDaChia.txt', 'w',encoding=\"utf-8\") as fp:\n",
    "    for x, y in zip(X_test, y_test):\n",
    "        fp.write('{} {}\\n'.format(y, x))\n",
    "\n",
    "# encode label\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print(list(label_encoder.classes_), '\\n')\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "print(X_train[0], y_train[0], '\\n')\n",
    "print(X_test[0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"models\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done training Naive Bayes in 1.100031852722168 seconds.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
    "                                             max_df=0.8,\n",
    "                                             max_features=None)), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print('Done training Naive Bayes in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"naive_bayes.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done training Linear Classifier in 2.5094542503356934 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
    "                                             max_df=0.8,\n",
    "                                             max_features=None)), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression(solver='lbfgs', \n",
    "                                                multi_class='auto',\n",
    "                                                max_iter=10000))\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print('Done training Linear Classifier in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"linear_classifier.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done training SVM in 172.15901708602905 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
    "                                             max_df=0.8,\n",
    "                                             max_features=None)), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SVC(gamma='scale'))\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print('Done training SVM in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"svm.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Naive Bayes, Accuracy = 0.6896742131419106\n",
      "Linear Classifier, Accuracy = 0.7109331860850359\n",
      "SVM, Accuracy = 0.7241855328547764\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Naive Bayes\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('Naive Bayes, Accuracy =', np.mean(y_pred == y_test))\n",
    "\n",
    "# Linear Classifier\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"linear_classifier.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('Linear Classifier, Accuracy =', np.mean(y_pred == y_test))\n",
    "\n",
    "# SVM\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"svm.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('SVM, Accuracy =', np.mean(y_pred == y_test))\n",
    "\n",
    "# Fasttext\n",
    "# def print_results(N, p, r):\n",
    "#     print(\"Fasttext, Precision = {}, Recall = {}\".format(p, r))\n",
    "    \n",
    "# model = fasttext.load_model(os.path.join(MODEL_PATH,\"fasttext.ftz\"))\n",
    "# print_results(*model.test('test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}